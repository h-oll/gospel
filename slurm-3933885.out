==========================================
SLURM_JOB_ID = 3933885
SLURM_JOB_NODELIST = node048
==========================================
2025-03-16 14:45:36,732 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.48:43947'
2025-03-16 14:45:36,740 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.48:39691'
2025-03-16 14:45:36,747 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.48:36493'
2025-03-16 14:45:36,761 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.48:37243'
2025-03-16 14:45:38,839 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-rsop6uct', purging
2025-03-16 14:45:38,841 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-4p2zs_wm', purging
2025-03-16 14:45:38,841 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ywzbz3py', purging
2025-03-16 14:45:38,842 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-tt24vivg', purging
2025-03-16 14:45:38,842 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-_rt_12hh', purging
2025-03-16 14:45:38,843 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-u7_cqcs9', purging
2025-03-16 14:45:38,844 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-v2pf3qw2', purging
2025-03-16 14:45:38,844 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-yq3xj7vy', purging
2025-03-16 14:45:38,864 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.48:45229
2025-03-16 14:45:38,865 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.48:45229
2025-03-16 14:45:38,865 - distributed.worker - INFO -           Worker name:          SLURMCluster-22-0
2025-03-16 14:45:38,865 - distributed.worker - INFO -          dashboard at:          10.224.1.48:36993
2025-03-16 14:45:38,865 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:43991
2025-03-16 14:45:38,865 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:38,865 - distributed.worker - INFO -               Threads:                          1
2025-03-16 14:45:38,866 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 14:45:38,866 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-nzjokuas
2025-03-16 14:45:38,866 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:38,918 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.48:35147
2025-03-16 14:45:38,919 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.48:35147
2025-03-16 14:45:38,919 - distributed.worker - INFO -           Worker name:          SLURMCluster-22-1
2025-03-16 14:45:38,919 - distributed.worker - INFO -          dashboard at:          10.224.1.48:42061
2025-03-16 14:45:38,919 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:43991
2025-03-16 14:45:38,919 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:38,919 - distributed.worker - INFO -               Threads:                          1
2025-03-16 14:45:38,920 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 14:45:38,920 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-mjry5og0
2025-03-16 14:45:38,920 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:38,926 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.48:39855
2025-03-16 14:45:38,927 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.48:39855
2025-03-16 14:45:38,927 - distributed.worker - INFO -           Worker name:          SLURMCluster-22-2
2025-03-16 14:45:38,927 - distributed.worker - INFO -          dashboard at:          10.224.1.48:37761
2025-03-16 14:45:38,927 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:43991
2025-03-16 14:45:38,928 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:38,928 - distributed.worker - INFO -               Threads:                          1
2025-03-16 14:45:38,928 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 14:45:38,928 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bgcah1f_
2025-03-16 14:45:38,929 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:38,942 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.48:43279
2025-03-16 14:45:38,942 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.48:43279
2025-03-16 14:45:38,943 - distributed.worker - INFO -           Worker name:          SLURMCluster-22-3
2025-03-16 14:45:38,943 - distributed.worker - INFO -          dashboard at:          10.224.1.48:46757
2025-03-16 14:45:38,943 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:43991
2025-03-16 14:45:38,943 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:38,943 - distributed.worker - INFO -               Threads:                          1
2025-03-16 14:45:38,944 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 14:45:38,944 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-d67d5vax
2025-03-16 14:45:38,944 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:39,801 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 14:45:39,803 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:43991
2025-03-16 14:45:39,803 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:39,803 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 14:45:39,804 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:43991
2025-03-16 14:45:39,805 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:39,805 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:43991
2025-03-16 14:45:39,805 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 14:45:39,806 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:43991
2025-03-16 14:45:39,806 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 14:45:39,807 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:43991
2025-03-16 14:45:39,807 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:39,808 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:43991
2025-03-16 14:45:39,808 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:39,808 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:43991
2025-03-16 14:45:39,809 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:43991
slurmstepd-node048: error: *** JOB 3933885 ON node048 CANCELLED AT 2025-03-16T14:45:47 ***
