==========================================
SLURM_JOB_ID = 3933835
SLURM_JOB_NODELIST = node048
==========================================
2025-03-16 14:43:44,006 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.48:33157'
2025-03-16 14:43:44,015 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.48:45995'
2025-03-16 14:43:44,020 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.48:42541'
2025-03-16 14:43:44,028 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.48:33913'
2025-03-16 14:43:46,159 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-s23nl7lw', purging
2025-03-16 14:43:46,160 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-o65w9v9l', purging
2025-03-16 14:43:46,161 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-_eydryth', purging
2025-03-16 14:43:46,162 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-4554ygu7', purging
2025-03-16 14:43:46,162 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-zs773n_p', purging
2025-03-16 14:43:46,163 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-5x3guno5', purging
2025-03-16 14:43:46,163 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-x25uxaat', purging
2025-03-16 14:43:46,164 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-njv81c8n', purging
2025-03-16 14:43:46,186 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.48:42273
2025-03-16 14:43:46,186 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.48:42273
2025-03-16 14:43:46,186 - distributed.worker - INFO -           Worker name:           SLURMCluster-8-1
2025-03-16 14:43:46,187 - distributed.worker - INFO -          dashboard at:          10.224.1.48:40799
2025-03-16 14:43:46,187 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:45905
2025-03-16 14:43:46,186 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.48:33503
2025-03-16 14:43:46,187 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:43:46,187 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.48:33503
2025-03-16 14:43:46,187 - distributed.worker - INFO -           Worker name:           SLURMCluster-8-0
2025-03-16 14:43:46,187 - distributed.worker - INFO -               Threads:                          1
2025-03-16 14:43:46,187 - distributed.worker - INFO -          dashboard at:          10.224.1.48:36811
2025-03-16 14:43:46,188 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 14:43:46,188 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:45905
2025-03-16 14:43:46,188 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-6eviay03
2025-03-16 14:43:46,188 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:43:46,188 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:43:46,188 - distributed.worker - INFO -               Threads:                          1
2025-03-16 14:43:46,188 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 14:43:46,189 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-69k29fa9
2025-03-16 14:43:46,189 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:43:46,193 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.48:44159
2025-03-16 14:43:46,194 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.48:44159
2025-03-16 14:43:46,194 - distributed.worker - INFO -           Worker name:           SLURMCluster-8-2
2025-03-16 14:43:46,195 - distributed.worker - INFO -          dashboard at:          10.224.1.48:42883
2025-03-16 14:43:46,195 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:45905
2025-03-16 14:43:46,195 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:43:46,195 - distributed.worker - INFO -               Threads:                          1
2025-03-16 14:43:46,195 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 14:43:46,196 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-wi_n2o_q
2025-03-16 14:43:46,196 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:43:46,213 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.48:37201
2025-03-16 14:43:46,213 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.48:37201
2025-03-16 14:43:46,213 - distributed.worker - INFO -           Worker name:           SLURMCluster-8-3
2025-03-16 14:43:46,214 - distributed.worker - INFO -          dashboard at:          10.224.1.48:46597
2025-03-16 14:43:46,214 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:45905
2025-03-16 14:43:46,214 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:43:46,214 - distributed.worker - INFO -               Threads:                          1
2025-03-16 14:43:46,215 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 14:43:46,215 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-ldh3o2u7
2025-03-16 14:43:46,215 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:43:47,229 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 14:43:47,230 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:45905
2025-03-16 14:43:47,230 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 14:43:47,231 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:43:47,231 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 14:43:47,232 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:45905
2025-03-16 14:43:47,232 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:43:47,232 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:45905
2025-03-16 14:43:47,233 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:45905
2025-03-16 14:43:47,233 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:43:47,233 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:45905
2025-03-16 14:43:47,234 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 14:43:47,234 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:45905
2025-03-16 14:43:47,235 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:45905
2025-03-16 14:43:47,235 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:43:47,237 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:45905
slurmstepd-node048: error: *** JOB 3933835 ON node048 CANCELLED AT 2025-03-16T14:43:54 ***
