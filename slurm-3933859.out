==========================================
SLURM_JOB_ID = 3933859
SLURM_JOB_NODELIST = node048
==========================================
2025-03-16 14:45:08,613 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.48:36511'
2025-03-16 14:45:08,622 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.48:46595'
2025-03-16 14:45:08,628 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.48:42211'
2025-03-16 14:45:08,637 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.48:36953'
2025-03-16 14:45:10,753 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-xmb1fxps', purging
2025-03-16 14:45:10,754 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-mmo60biq', purging
2025-03-16 14:45:10,755 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ldh3o2u7', purging
2025-03-16 14:45:10,756 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-554rjywe', purging
2025-03-16 14:45:10,756 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-wi_n2o_q', purging
2025-03-16 14:45:10,757 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-2lj1n5bk', purging
2025-03-16 14:45:10,757 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-6eviay03', purging
2025-03-16 14:45:10,758 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-69k29fa9', purging
2025-03-16 14:45:10,781 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.48:35823
2025-03-16 14:45:10,781 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.48:35823
2025-03-16 14:45:10,781 - distributed.worker - INFO -           Worker name:           SLURMCluster-6-0
2025-03-16 14:45:10,781 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.48:40095
2025-03-16 14:45:10,782 - distributed.worker - INFO -          dashboard at:          10.224.1.48:35825
2025-03-16 14:45:10,782 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.48:40095
2025-03-16 14:45:10,782 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:38845
2025-03-16 14:45:10,782 - distributed.worker - INFO -           Worker name:           SLURMCluster-6-3
2025-03-16 14:45:10,782 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:10,782 - distributed.worker - INFO -          dashboard at:          10.224.1.48:34403
2025-03-16 14:45:10,782 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:38845
2025-03-16 14:45:10,782 - distributed.worker - INFO -               Threads:                          1
2025-03-16 14:45:10,782 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:10,782 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 14:45:10,783 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-v2pf3qw2
2025-03-16 14:45:10,783 - distributed.worker - INFO -               Threads:                          1
2025-03-16 14:45:10,783 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:10,783 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 14:45:10,783 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-yq3xj7vy
2025-03-16 14:45:10,783 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:10,815 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.48:42283
2025-03-16 14:45:10,815 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.48:42283
2025-03-16 14:45:10,815 - distributed.worker - INFO -           Worker name:           SLURMCluster-6-1
2025-03-16 14:45:10,816 - distributed.worker - INFO -          dashboard at:          10.224.1.48:44695
2025-03-16 14:45:10,816 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:38845
2025-03-16 14:45:10,816 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:10,816 - distributed.worker - INFO -               Threads:                          1
2025-03-16 14:45:10,816 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 14:45:10,816 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-4p2zs_wm
2025-03-16 14:45:10,817 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:10,856 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.48:38801
2025-03-16 14:45:10,856 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.48:38801
2025-03-16 14:45:10,856 - distributed.worker - INFO -           Worker name:           SLURMCluster-6-2
2025-03-16 14:45:10,856 - distributed.worker - INFO -          dashboard at:          10.224.1.48:35737
2025-03-16 14:45:10,856 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:38845
2025-03-16 14:45:10,857 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:10,857 - distributed.worker - INFO -               Threads:                          1
2025-03-16 14:45:10,857 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 14:45:10,857 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-rsop6uct
2025-03-16 14:45:10,857 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:11,992 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 14:45:11,993 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:38845
2025-03-16 14:45:11,993 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:11,993 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 14:45:11,995 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:38845
2025-03-16 14:45:11,995 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:38845
2025-03-16 14:45:11,995 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:11,998 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:38845
2025-03-16 14:45:12,037 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 14:45:12,039 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:38845
2025-03-16 14:45:12,039 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:12,041 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:38845
2025-03-16 14:45:12,840 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 14:45:12,842 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:38845
2025-03-16 14:45:12,843 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:45:12,844 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:38845
slurmstepd-node048: error: *** JOB 3933859 ON node048 CANCELLED AT 2025-03-16T14:45:15 ***
