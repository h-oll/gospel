==========================================
SLURM_JOB_ID = 3936182
SLURM_JOB_NODELIST = node036
==========================================
2025-03-17 00:32:18,916 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.36:45043'
2025-03-17 00:32:18,921 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.36:33507'
2025-03-17 00:32:18,926 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.36:46151'
2025-03-17 00:32:18,929 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.36:36881'
2025-03-17 00:32:20,111 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-m_5jbmti', purging
2025-03-17 00:32:20,112 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-0wfyttvz', purging
2025-03-17 00:32:20,112 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-9a6bjy54', purging
2025-03-17 00:32:20,113 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-_xv2i6ui', purging
2025-03-17 00:32:20,130 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.36:42855
2025-03-17 00:32:20,130 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.36:42855
2025-03-17 00:32:20,130 - distributed.worker - INFO -           Worker name:          SLURMCluster-18-1
2025-03-17 00:32:20,130 - distributed.worker - INFO -          dashboard at:          10.224.1.36:38847
2025-03-17 00:32:20,131 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:36073
2025-03-17 00:32:20,131 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:32:20,131 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:32:20,131 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:32:20,131 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-tojesgc0
2025-03-17 00:32:20,131 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:32:20,138 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.36:39463
2025-03-17 00:32:20,139 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.36:39463
2025-03-17 00:32:20,139 - distributed.worker - INFO -           Worker name:          SLURMCluster-18-2
2025-03-17 00:32:20,139 - distributed.worker - INFO -          dashboard at:          10.224.1.36:43555
2025-03-17 00:32:20,139 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:36073
2025-03-17 00:32:20,139 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:32:20,139 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:32:20,139 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:32:20,140 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-f6mziing
2025-03-17 00:32:20,140 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:32:20,389 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.36:37621
2025-03-17 00:32:20,389 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.36:37621
2025-03-17 00:32:20,389 - distributed.worker - INFO -           Worker name:          SLURMCluster-18-3
2025-03-17 00:32:20,389 - distributed.worker - INFO -          dashboard at:          10.224.1.36:32861
2025-03-17 00:32:20,390 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:36073
2025-03-17 00:32:20,390 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:32:20,390 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:32:20,390 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:32:20,390 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bwsele75
2025-03-17 00:32:20,390 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:32:20,395 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.36:40983
2025-03-17 00:32:20,395 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.36:40983
2025-03-17 00:32:20,396 - distributed.worker - INFO -           Worker name:          SLURMCluster-18-0
2025-03-17 00:32:20,396 - distributed.worker - INFO -          dashboard at:          10.224.1.36:41285
2025-03-17 00:32:20,396 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:36073
2025-03-17 00:32:20,396 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:32:20,396 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:32:20,396 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:32:20,396 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-9mry0drc
2025-03-17 00:32:20,396 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:32:20,686 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:32:20,687 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:36073
2025-03-17 00:32:20,687 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:32:20,687 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:32:20,687 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:36073
2025-03-17 00:32:20,687 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:36073
2025-03-17 00:32:20,688 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:32:20,688 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:36073
2025-03-17 00:32:20,769 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:32:20,769 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:36073
2025-03-17 00:32:20,770 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:32:20,770 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:36073
2025-03-17 00:32:20,773 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:32:20,774 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:36073
2025-03-17 00:32:20,774 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:32:20,774 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:36073
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:57: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:57: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:39: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:39: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:76: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:76: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:57: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:39: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:76: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:57: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:39: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:76: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
2025-03-17 00:32:25,341 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.43s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-03-17 00:32:25,358 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.44s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-03-17 00:32:25,811 - distributed.core - INFO - Event loop was unresponsive in Worker for 3.90s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-03-17 00:32:26,031 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.14s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
