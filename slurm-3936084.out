==========================================
SLURM_JOB_ID = 3936084
SLURM_JOB_NODELIST = node047
==========================================
2025-03-16 22:42:12,536 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.47:34857'
2025-03-16 22:42:12,548 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.47:42765'
2025-03-16 22:42:12,555 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.47:33987'
2025-03-16 22:42:12,564 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.47:38863'
2025-03-16 22:42:14,818 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.47:40713
2025-03-16 22:42:14,818 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.47:40713
2025-03-16 22:42:14,818 - distributed.worker - INFO -           Worker name:           SLURMCluster-2-0
2025-03-16 22:42:14,819 - distributed.worker - INFO -          dashboard at:          10.224.1.47:33011
2025-03-16 22:42:14,819 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:39247
2025-03-16 22:42:14,819 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 22:42:14,819 - distributed.worker - INFO -               Threads:                          1
2025-03-16 22:42:14,820 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 22:42:14,820 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-9wbibr10
2025-03-16 22:42:14,820 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 22:42:14,841 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.47:33261
2025-03-16 22:42:14,841 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.47:33261
2025-03-16 22:42:14,841 - distributed.worker - INFO -           Worker name:           SLURMCluster-2-1
2025-03-16 22:42:14,841 - distributed.worker - INFO -          dashboard at:          10.224.1.47:39231
2025-03-16 22:42:14,841 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:39247
2025-03-16 22:42:14,842 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 22:42:14,842 - distributed.worker - INFO -               Threads:                          1
2025-03-16 22:42:14,842 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 22:42:14,842 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-ux21ryjv
2025-03-16 22:42:14,842 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 22:42:14,860 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.47:36049
2025-03-16 22:42:14,860 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.47:36049
2025-03-16 22:42:14,861 - distributed.worker - INFO -           Worker name:           SLURMCluster-2-3
2025-03-16 22:42:14,861 - distributed.worker - INFO -          dashboard at:          10.224.1.47:39965
2025-03-16 22:42:14,861 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:39247
2025-03-16 22:42:14,861 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 22:42:14,861 - distributed.worker - INFO -               Threads:                          1
2025-03-16 22:42:14,862 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 22:42:14,862 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-sn94rem3
2025-03-16 22:42:14,862 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 22:42:14,874 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.47:44031
2025-03-16 22:42:14,874 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.47:44031
2025-03-16 22:42:14,874 - distributed.worker - INFO -           Worker name:           SLURMCluster-2-2
2025-03-16 22:42:14,874 - distributed.worker - INFO -          dashboard at:          10.224.1.47:34377
2025-03-16 22:42:14,874 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:39247
2025-03-16 22:42:14,875 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 22:42:14,875 - distributed.worker - INFO -               Threads:                          1
2025-03-16 22:42:14,875 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 22:42:14,875 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-4rbj_nye
2025-03-16 22:42:14,875 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 22:42:29,477 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 22:42:29,479 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:39247
2025-03-16 22:42:29,479 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 22:42:29,480 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:39247
2025-03-16 22:42:29,518 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 22:42:29,519 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:39247
2025-03-16 22:42:29,519 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 22:42:29,521 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:39247
2025-03-16 22:42:29,564 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 22:42:29,566 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:39247
2025-03-16 22:42:29,566 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 22:42:29,567 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:39247
2025-03-16 22:42:29,651 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 22:42:29,653 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:39247
2025-03-16 22:42:29,653 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 22:42:29,654 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:39247
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:57: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:39: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:57: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:39: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:76: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:57: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:76: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:39: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:76: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:57: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:39: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:76: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
2025-03-16 22:44:23,006 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-03-16 22:44:23,018 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-03-16 22:44:23,023 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.56s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-03-16 22:44:23,039 - distributed.core - INFO - Event loop was unresponsive in Worker for 9.60s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
slurmstepd-node047: error: *** JOB 3936084 ON node047 CANCELLED AT 2025-03-17T00:24:23 ***
