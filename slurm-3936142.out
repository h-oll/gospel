==========================================
SLURM_JOB_ID = 3936142
SLURM_JOB_NODELIST = node047
==========================================
2025-03-17 00:22:47,191 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.47:42995'
2025-03-17 00:22:47,202 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.47:40199'
2025-03-17 00:22:47,208 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.47:41839'
2025-03-17 00:22:47,216 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.47:43743'
2025-03-17 00:22:49,423 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-wrkqjiv0', purging
2025-03-17 00:22:49,424 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-q5osfsmo', purging
2025-03-17 00:22:49,425 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-e6ihvsk0', purging
2025-03-17 00:22:49,425 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-3aceyd4z', purging
2025-03-17 00:22:49,451 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.47:46127
2025-03-17 00:22:49,451 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.47:46127
2025-03-17 00:22:49,451 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.47:35571
2025-03-17 00:22:49,451 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-1
2025-03-17 00:22:49,451 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.47:35571
2025-03-17 00:22:49,451 - distributed.worker - INFO -          dashboard at:          10.224.1.47:41957
2025-03-17 00:22:49,451 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-2
2025-03-17 00:22:49,452 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:33819
2025-03-17 00:22:49,452 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:49,452 - distributed.worker - INFO -          dashboard at:          10.224.1.47:33209
2025-03-17 00:22:49,452 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:33819
2025-03-17 00:22:49,452 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:49,452 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:22:49,452 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:22:49,452 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-6x39d6qw
2025-03-17 00:22:49,452 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:22:49,453 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:49,453 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:22:49,453 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-sij1n1d8
2025-03-17 00:22:49,453 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:49,502 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.47:41115
2025-03-17 00:22:49,503 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.47:41115
2025-03-17 00:22:49,503 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-0
2025-03-17 00:22:49,503 - distributed.worker - INFO -          dashboard at:          10.224.1.47:46137
2025-03-17 00:22:49,503 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:33819
2025-03-17 00:22:49,503 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:49,504 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:22:49,504 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:22:49,504 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-z4z_jkv8
2025-03-17 00:22:49,505 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:49,543 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.47:45857
2025-03-17 00:22:49,544 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.47:45857
2025-03-17 00:22:49,544 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-3
2025-03-17 00:22:49,544 - distributed.worker - INFO -          dashboard at:          10.224.1.47:34247
2025-03-17 00:22:49,544 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:33819
2025-03-17 00:22:49,544 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:49,544 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:22:49,545 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:22:49,545 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-v673kde2
2025-03-17 00:22:49,545 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:50,511 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:22:50,511 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:22:50,512 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:33819
2025-03-17 00:22:50,513 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:50,512 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:22:50,514 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:33819
2025-03-17 00:22:50,514 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:50,514 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:33819
2025-03-17 00:22:50,514 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:33819
2025-03-17 00:22:50,514 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:50,515 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:33819
2025-03-17 00:22:50,516 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:33819
2025-03-17 00:22:50,528 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:22:50,530 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:33819
2025-03-17 00:22:50,531 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:50,533 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:33819
slurmstepd-node047: error: *** JOB 3936142 ON node047 CANCELLED AT 2025-03-17T00:32:06 ***
