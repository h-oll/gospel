==========================================
SLURM_JOB_ID = 3933762
SLURM_JOB_NODELIST = node027
==========================================
2025-03-16 14:21:26,181 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.27:41827'
2025-03-16 14:21:26,190 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.27:34121'
2025-03-16 14:21:26,193 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.27:46847'
2025-03-16 14:21:26,199 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.27:34233'
2025-03-16 14:21:27,063 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-xs6eo3rz', purging
2025-03-16 14:21:27,064 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-cxzheckp', purging
2025-03-16 14:21:27,064 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-rtq8j8ef', purging
2025-03-16 14:21:27,065 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-iskb292y', purging
2025-03-16 14:21:27,065 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-4svjqlgr', purging
2025-03-16 14:21:27,065 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-27augoq_', purging
2025-03-16 14:21:27,065 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-puhnn53g', purging
2025-03-16 14:21:27,065 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-amzr1o8b', purging
2025-03-16 14:21:27,066 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-r5mldiq3', purging
2025-03-16 14:21:27,066 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-x_qoyv4w', purging
2025-03-16 14:21:27,066 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-_2pcj6m3', purging
2025-03-16 14:21:27,066 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-k7y6229w', purging
2025-03-16 14:21:27,066 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-wtzwni39', purging
2025-03-16 14:21:27,067 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-ck814qye', purging
2025-03-16 14:21:27,067 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-gks1i6fy', purging
2025-03-16 14:21:27,067 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-sc9kxdae', purging
2025-03-16 14:21:27,076 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.27:34391
2025-03-16 14:21:27,076 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.27:34391
2025-03-16 14:21:27,076 - distributed.worker - INFO -           Worker name:          SLURMCluster-11-0
2025-03-16 14:21:27,077 - distributed.worker - INFO -          dashboard at:          10.224.1.27:40473
2025-03-16 14:21:27,077 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:41779
2025-03-16 14:21:27,076 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.27:45837
2025-03-16 14:21:27,077 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:27,077 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.27:45837
2025-03-16 14:21:27,077 - distributed.worker - INFO -           Worker name:          SLURMCluster-11-1
2025-03-16 14:21:27,077 - distributed.worker - INFO -          dashboard at:          10.224.1.27:40739
2025-03-16 14:21:27,077 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:41779
2025-03-16 14:21:27,077 - distributed.worker - INFO -               Threads:                          1
2025-03-16 14:21:27,077 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:27,077 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 14:21:27,077 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-0d20y60u
2025-03-16 14:21:27,077 - distributed.worker - INFO -               Threads:                          1
2025-03-16 14:21:27,077 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:27,077 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 14:21:27,077 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-2jqgbb35
2025-03-16 14:21:27,077 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:27,129 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.27:33373
2025-03-16 14:21:27,129 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.27:33373
2025-03-16 14:21:27,130 - distributed.worker - INFO -           Worker name:          SLURMCluster-11-2
2025-03-16 14:21:27,130 - distributed.worker - INFO -          dashboard at:          10.224.1.27:35235
2025-03-16 14:21:27,130 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:41779
2025-03-16 14:21:27,130 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:27,130 - distributed.worker - INFO -               Threads:                          1
2025-03-16 14:21:27,130 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 14:21:27,130 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-d00wycx_
2025-03-16 14:21:27,130 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:27,134 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.27:45097
2025-03-16 14:21:27,134 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.27:45097
2025-03-16 14:21:27,134 - distributed.worker - INFO -           Worker name:          SLURMCluster-11-3
2025-03-16 14:21:27,134 - distributed.worker - INFO -          dashboard at:          10.224.1.27:38237
2025-03-16 14:21:27,134 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:41779
2025-03-16 14:21:27,134 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:27,134 - distributed.worker - INFO -               Threads:                          1
2025-03-16 14:21:27,135 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 14:21:27,135 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-66iu8s5f
2025-03-16 14:21:27,135 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:27,566 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 14:21:27,567 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:41779
2025-03-16 14:21:27,567 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:27,568 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:41779
2025-03-16 14:21:27,569 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 14:21:27,569 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:41779
2025-03-16 14:21:27,569 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:27,570 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:41779
2025-03-16 14:21:27,570 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 14:21:27,571 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:41779
2025-03-16 14:21:27,571 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:27,572 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:41779
2025-03-16 14:21:27,572 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 14:21:27,572 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:41779
2025-03-16 14:21:27,573 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:27,573 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:41779
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:57: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:39: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:76: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
slurmstepd-node027: error: *** JOB 3933762 ON node027 CANCELLED AT 2025-03-16T14:21:42 ***
