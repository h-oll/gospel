==========================================
SLURM_JOB_ID = 3936137
SLURM_JOB_NODELIST = node038
==========================================
2025-03-17 00:21:46,719 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.38:33139'
2025-03-17 00:21:46,724 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.38:36861'
2025-03-17 00:21:46,727 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.38:40411'
2025-03-17 00:21:46,738 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.38:40011'
2025-03-17 00:21:47,891 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.38:34353
2025-03-17 00:21:47,891 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.38:34353
2025-03-17 00:21:47,891 - distributed.worker - INFO -           Worker name:           SLURMCluster-5-2
2025-03-17 00:21:47,892 - distributed.worker - INFO -          dashboard at:          10.224.1.38:42001
2025-03-17 00:21:47,892 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:33497
2025-03-17 00:21:47,892 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:21:47,892 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:21:47,892 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:21:47,892 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-w9vmg7cv
2025-03-17 00:21:47,892 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:21:47,895 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.38:42709
2025-03-17 00:21:47,896 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.38:42709
2025-03-17 00:21:47,896 - distributed.worker - INFO -           Worker name:           SLURMCluster-5-1
2025-03-17 00:21:47,896 - distributed.worker - INFO -          dashboard at:          10.224.1.38:33783
2025-03-17 00:21:47,896 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:33497
2025-03-17 00:21:47,896 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:21:47,896 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:21:47,896 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:21:47,896 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-8184imwt
2025-03-17 00:21:47,897 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:21:48,189 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.38:46627
2025-03-17 00:21:48,190 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.38:46627
2025-03-17 00:21:48,190 - distributed.worker - INFO -           Worker name:           SLURMCluster-5-0
2025-03-17 00:21:48,190 - distributed.worker - INFO -          dashboard at:          10.224.1.38:40027
2025-03-17 00:21:48,190 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:33497
2025-03-17 00:21:48,190 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:21:48,190 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:21:48,190 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:21:48,190 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-2mgk16vq
2025-03-17 00:21:48,190 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:21:48,204 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.38:36233
2025-03-17 00:21:48,204 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.38:36233
2025-03-17 00:21:48,204 - distributed.worker - INFO -           Worker name:           SLURMCluster-5-3
2025-03-17 00:21:48,204 - distributed.worker - INFO -          dashboard at:          10.224.1.38:35945
2025-03-17 00:21:48,205 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:33497
2025-03-17 00:21:48,205 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:21:48,205 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:21:48,205 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:21:48,205 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-vgdcp5yp
2025-03-17 00:21:48,205 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:21:48,408 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:21:48,408 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:33497
2025-03-17 00:21:48,408 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:21:48,409 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:33497
2025-03-17 00:21:48,409 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:21:48,410 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:33497
2025-03-17 00:21:48,410 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:21:48,411 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:33497
2025-03-17 00:21:48,617 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:21:48,618 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:33497
2025-03-17 00:21:48,618 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:21:48,620 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:33497
2025-03-17 00:21:48,627 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:21:48,627 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:33497
2025-03-17 00:21:48,627 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:21:48,628 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:33497
slurmstepd-node038: error: *** JOB 3936137 ON node038 CANCELLED AT 2025-03-17T00:22:29 ***
