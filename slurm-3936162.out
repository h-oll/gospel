==========================================
SLURM_JOB_ID = 3936162
SLURM_JOB_NODELIST = node051
==========================================
2025-03-17 00:24:47,182 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.51:40459'
2025-03-17 00:24:47,187 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.51:33493'
2025-03-17 00:24:47,191 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.51:35901'
2025-03-17 00:24:47,195 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.51:33959'
2025-03-17 00:24:48,055 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-nj6lfzd_', purging
2025-03-17 00:24:48,056 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-5za7eben', purging
2025-03-17 00:24:48,056 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-gr67kffb', purging
2025-03-17 00:24:48,056 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-4zdypmu2', purging
2025-03-17 00:24:48,073 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.51:33949
2025-03-17 00:24:48,073 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.51:33949
2025-03-17 00:24:48,073 - distributed.worker - INFO -           Worker name:           SLURMCluster-6-1
2025-03-17 00:24:48,073 - distributed.worker - INFO -          dashboard at:          10.224.1.51:40171
2025-03-17 00:24:48,073 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:36029
2025-03-17 00:24:48,073 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:24:48,073 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:24:48,074 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:24:48,074 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-muzuuktb
2025-03-17 00:24:48,074 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:24:48,120 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.51:41873
2025-03-17 00:24:48,120 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.51:41873
2025-03-17 00:24:48,120 - distributed.worker - INFO -           Worker name:           SLURMCluster-6-0
2025-03-17 00:24:48,120 - distributed.worker - INFO -          dashboard at:          10.224.1.51:41841
2025-03-17 00:24:48,120 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:36029
2025-03-17 00:24:48,120 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:24:48,120 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:24:48,121 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:24:48,121 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-91h19nxv
2025-03-17 00:24:48,121 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:24:48,223 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.51:35969
2025-03-17 00:24:48,223 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.51:35969
2025-03-17 00:24:48,223 - distributed.worker - INFO -           Worker name:           SLURMCluster-6-3
2025-03-17 00:24:48,223 - distributed.worker - INFO -          dashboard at:          10.224.1.51:43215
2025-03-17 00:24:48,223 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:36029
2025-03-17 00:24:48,223 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:24:48,223 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:24:48,223 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:24:48,223 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-lxq7ww_r
2025-03-17 00:24:48,223 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:24:48,233 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.51:37657
2025-03-17 00:24:48,233 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.51:37657
2025-03-17 00:24:48,233 - distributed.worker - INFO -           Worker name:           SLURMCluster-6-2
2025-03-17 00:24:48,233 - distributed.worker - INFO -          dashboard at:          10.224.1.51:39667
2025-03-17 00:24:48,233 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:36029
2025-03-17 00:24:48,233 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:24:48,233 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:24:48,233 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:24:48,233 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-ifev1glm
2025-03-17 00:24:48,233 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:24:48,483 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:24:48,484 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:36029
2025-03-17 00:24:48,484 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:24:48,484 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:24:48,485 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:36029
2025-03-17 00:24:48,485 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:36029
2025-03-17 00:24:48,485 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:24:48,485 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:36029
2025-03-17 00:24:48,516 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:24:48,517 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:36029
2025-03-17 00:24:48,517 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:24:48,518 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:36029
2025-03-17 00:24:48,525 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:24:48,526 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:36029
2025-03-17 00:24:48,526 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:24:48,526 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:36029
slurmstepd-node051: error: *** JOB 3936162 ON node051 CANCELLED AT 2025-03-17T00:40:56 ***
