==========================================
SLURM_JOB_ID = 3936138
SLURM_JOB_NODELIST = node029
==========================================
2025-03-17 00:22:46,578 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.29:43483'
2025-03-17 00:22:46,584 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.29:34101'
2025-03-17 00:22:46,586 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.29:40579'
2025-03-17 00:22:46,606 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.29:43825'
2025-03-17 00:22:47,716 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-jy6vsywv', purging
2025-03-17 00:22:47,717 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-xvln5ujz', purging
2025-03-17 00:22:47,717 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-c_h9o5ah', purging
2025-03-17 00:22:47,718 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-e4jqq34k', purging
2025-03-17 00:22:47,734 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.29:38585
2025-03-17 00:22:47,734 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.29:38585
2025-03-17 00:22:47,734 - distributed.worker - INFO -           Worker name:           SLURMCluster-8-1
2025-03-17 00:22:47,734 - distributed.worker - INFO -          dashboard at:          10.224.1.29:34591
2025-03-17 00:22:47,734 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:32811
2025-03-17 00:22:47,734 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:47,734 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:22:47,735 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:22:47,735 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-z4qbha4r
2025-03-17 00:22:47,735 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:47,738 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.29:35833
2025-03-17 00:22:47,739 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.29:35833
2025-03-17 00:22:47,739 - distributed.worker - INFO -           Worker name:           SLURMCluster-8-0
2025-03-17 00:22:47,739 - distributed.worker - INFO -          dashboard at:          10.224.1.29:41109
2025-03-17 00:22:47,739 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:32811
2025-03-17 00:22:47,739 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:47,739 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:22:47,739 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:22:47,739 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-5f7h3ph2
2025-03-17 00:22:47,739 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:48,021 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.29:39041
2025-03-17 00:22:48,021 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.29:39041
2025-03-17 00:22:48,021 - distributed.worker - INFO -           Worker name:           SLURMCluster-8-3
2025-03-17 00:22:48,021 - distributed.worker - INFO -          dashboard at:          10.224.1.29:38321
2025-03-17 00:22:48,021 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:32811
2025-03-17 00:22:48,021 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:48,021 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:22:48,021 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:22:48,022 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-kjqeq4ln
2025-03-17 00:22:48,022 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:48,029 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.29:35341
2025-03-17 00:22:48,029 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.29:35341
2025-03-17 00:22:48,030 - distributed.worker - INFO -           Worker name:           SLURMCluster-8-2
2025-03-17 00:22:48,030 - distributed.worker - INFO -          dashboard at:          10.224.1.29:39139
2025-03-17 00:22:48,030 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:32811
2025-03-17 00:22:48,030 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:48,030 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:22:48,030 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:22:48,030 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-30bbhsxc
2025-03-17 00:22:48,030 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:48,273 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:22:48,274 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:32811
2025-03-17 00:22:48,274 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:22:48,274 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:48,274 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:32811
2025-03-17 00:22:48,275 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:48,275 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:32811
2025-03-17 00:22:48,275 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:32811
2025-03-17 00:22:48,376 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:22:48,376 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:32811
2025-03-17 00:22:48,376 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:48,377 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:32811
2025-03-17 00:22:48,380 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:22:48,381 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:32811
2025-03-17 00:22:48,381 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:48,382 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:32811
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:57: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:39: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:57: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:39: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:76: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:76: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:57: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:39: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:76: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:57: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:39: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.
  warnings.warn(
/home/sabdulsa/miniconda3/envs/gospel/lib/python3.12/site-packages/cotengra/hyperoptimizers/hyper.py:76: UserWarning: Couldn't find `optuna`, `cmaes`, or `nevergrad` so will use completely random sampling in place of hyper-optimization.
  warnings.warn(
2025-03-17 00:22:53,707 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.81s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-03-17 00:22:53,722 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.84s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-03-17 00:22:53,931 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.05s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2025-03-17 00:22:54,016 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.12s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
