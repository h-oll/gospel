==========================================
SLURM_JOB_ID = 3933784
SLURM_JOB_NODELIST = node048
==========================================
2025-03-16 14:21:29,236 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.48:36357'
2025-03-16 14:21:29,244 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.48:38299'
2025-03-16 14:21:29,250 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.48:38535'
2025-03-16 14:21:29,261 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.48:45081'
2025-03-16 14:21:31,374 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-uc22wri9', purging
2025-03-16 14:21:31,375 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-7jfe9xoy', purging
2025-03-16 14:21:31,376 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-bwco60ti', purging
2025-03-16 14:21:31,377 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-ghag4l1r', purging
2025-03-16 14:21:31,377 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-jbf0kjnn', purging
2025-03-16 14:21:31,378 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-1nzskkir', purging
2025-03-16 14:21:31,378 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-x152cfg3', purging
2025-03-16 14:21:31,379 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space/worker-oczq1gpi', purging
2025-03-16 14:21:31,407 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.48:46207
2025-03-16 14:21:31,408 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.48:46207
2025-03-16 14:21:31,408 - distributed.worker - INFO -           Worker name:           SLURMCluster-5-0
2025-03-16 14:21:31,408 - distributed.worker - INFO -          dashboard at:          10.224.1.48:39885
2025-03-16 14:21:31,408 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:41779
2025-03-16 14:21:31,408 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:31,408 - distributed.worker - INFO -               Threads:                          1
2025-03-16 14:21:31,409 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 14:21:31,409 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-kkzlqqdz
2025-03-16 14:21:31,409 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:31,432 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.48:46393
2025-03-16 14:21:31,432 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.48:46393
2025-03-16 14:21:31,432 - distributed.worker - INFO -           Worker name:           SLURMCluster-5-2
2025-03-16 14:21:31,433 - distributed.worker - INFO -          dashboard at:          10.224.1.48:41317
2025-03-16 14:21:31,433 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:41779
2025-03-16 14:21:31,433 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:31,433 - distributed.worker - INFO -               Threads:                          1
2025-03-16 14:21:31,433 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 14:21:31,434 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-bs1l031t
2025-03-16 14:21:31,434 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:31,469 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.48:33609
2025-03-16 14:21:31,470 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.48:33609
2025-03-16 14:21:31,470 - distributed.worker - INFO -           Worker name:           SLURMCluster-5-1
2025-03-16 14:21:31,470 - distributed.worker - INFO -          dashboard at:          10.224.1.48:37319
2025-03-16 14:21:31,470 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:41779
2025-03-16 14:21:31,470 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:31,471 - distributed.worker - INFO -               Threads:                          1
2025-03-16 14:21:31,471 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 14:21:31,471 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-_ooh30m7
2025-03-16 14:21:31,471 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:31,491 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.48:42173
2025-03-16 14:21:31,492 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.48:42173
2025-03-16 14:21:31,492 - distributed.worker - INFO -           Worker name:           SLURMCluster-5-3
2025-03-16 14:21:31,492 - distributed.worker - INFO -          dashboard at:          10.224.1.48:40449
2025-03-16 14:21:31,492 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:41779
2025-03-16 14:21:31,492 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:31,493 - distributed.worker - INFO -               Threads:                          1
2025-03-16 14:21:31,493 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-16 14:21:31,493 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space/worker-p66beqaf
2025-03-16 14:21:31,493 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:32,472 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 14:21:32,474 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:41779
2025-03-16 14:21:32,474 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:32,474 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 14:21:32,475 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:41779
2025-03-16 14:21:32,476 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:32,476 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:41779
2025-03-16 14:21:32,477 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 14:21:32,479 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:41779
2025-03-16 14:21:32,479 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:32,480 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:41779
2025-03-16 14:21:32,481 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:41779
2025-03-16 14:21:32,483 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-16 14:21:32,485 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:41779
2025-03-16 14:21:32,485 - distributed.worker - INFO - -------------------------------------------------
2025-03-16 14:21:32,487 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:41779
slurmstepd-node048: error: *** JOB 3933784 ON node048 CANCELLED AT 2025-03-16T14:21:42 ***
