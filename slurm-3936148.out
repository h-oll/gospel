==========================================
SLURM_JOB_ID = 3936148
SLURM_JOB_NODELIST = node039
==========================================
2025-03-17 00:22:46,587 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.39:42579'
2025-03-17 00:22:46,593 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.39:41205'
2025-03-17 00:22:46,597 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.39:34643'
2025-03-17 00:22:46,610 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.39:35895'
2025-03-17 00:22:47,734 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-k4y73466', purging
2025-03-17 00:22:47,735 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-m6_cxey_', purging
2025-03-17 00:22:47,735 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-9vn5rx55', purging
2025-03-17 00:22:47,735 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-sj4ozuua', purging
2025-03-17 00:22:47,751 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.39:41015
2025-03-17 00:22:47,751 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.39:41015
2025-03-17 00:22:47,751 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-1
2025-03-17 00:22:47,751 - distributed.worker - INFO -          dashboard at:          10.224.1.39:39267
2025-03-17 00:22:47,751 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:36029
2025-03-17 00:22:47,751 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:47,751 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:22:47,752 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:22:47,752 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-s978dabf
2025-03-17 00:22:47,752 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:47,755 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.39:38811
2025-03-17 00:22:47,755 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.39:38811
2025-03-17 00:22:47,755 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-0
2025-03-17 00:22:47,755 - distributed.worker - INFO -          dashboard at:          10.224.1.39:38681
2025-03-17 00:22:47,755 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:36029
2025-03-17 00:22:47,755 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:47,755 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:22:47,755 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:22:47,755 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-wy4rlxhd
2025-03-17 00:22:47,755 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:47,990 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.39:45849
2025-03-17 00:22:47,990 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.39:45849
2025-03-17 00:22:47,991 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-2
2025-03-17 00:22:47,991 - distributed.worker - INFO -          dashboard at:          10.224.1.39:41545
2025-03-17 00:22:47,991 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:36029
2025-03-17 00:22:47,991 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:47,991 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:22:47,991 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:22:47,991 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-yeqw644t
2025-03-17 00:22:47,991 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:48,011 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.39:34767
2025-03-17 00:22:48,011 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.39:34767
2025-03-17 00:22:48,011 - distributed.worker - INFO -           Worker name:           SLURMCluster-1-3
2025-03-17 00:22:48,011 - distributed.worker - INFO -          dashboard at:          10.224.1.39:45109
2025-03-17 00:22:48,012 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:36029
2025-03-17 00:22:48,012 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:48,012 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:22:48,012 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:22:48,012 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-cok4cwz2
2025-03-17 00:22:48,012 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:48,262 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:22:48,262 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:36029
2025-03-17 00:22:48,262 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:48,263 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:36029
2025-03-17 00:22:48,263 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:22:48,264 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:36029
2025-03-17 00:22:48,264 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:48,264 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:36029
2025-03-17 00:22:48,406 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:22:48,407 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:36029
2025-03-17 00:22:48,407 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:48,408 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:36029
2025-03-17 00:22:48,427 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:22:48,427 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:36029
2025-03-17 00:22:48,427 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:22:48,428 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:36029
slurmstepd-node039: error: *** JOB 3936148 ON node039 CANCELLED AT 2025-03-17T00:40:56 ***
2025-03-17 00:40:56,933 - distributed.worker - INFO - Stopping worker at tcp://10.224.1.39:34767. Reason: scheduler-close
2025-03-17 00:40:56,933 - distributed.worker - INFO - Stopping worker at tcp://10.224.1.39:38811. Reason: scheduler-close
2025-03-17 00:40:56,933 - distributed.worker - INFO - Stopping worker at tcp://10.224.1.39:41015. Reason: scheduler-close
2025-03-17 00:40:56,933 - distributed.worker - INFO - Stopping worker at tcp://10.224.1.39:45849. Reason: scheduler-close
