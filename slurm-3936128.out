==========================================
SLURM_JOB_ID = 3936128
SLURM_JOB_NODELIST = node038
==========================================
2025-03-17 00:02:35,322 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.38:36511'
2025-03-17 00:02:35,329 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.38:39907'
2025-03-17 00:02:35,333 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.38:44483'
2025-03-17 00:02:35,339 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.224.1.38:44053'
2025-03-17 00:02:36,572 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-j26x817g', purging
2025-03-17 00:02:36,573 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-s6hb9bdw', purging
2025-03-17 00:02:36,573 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-6yqbywtf', purging
2025-03-17 00:02:36,573 - distributed.diskutils - INFO - Found stale lock file and directory '/tmp/dask-scratch-space-672848/worker-rfpq8_2p', purging
2025-03-17 00:02:36,592 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.38:43911
2025-03-17 00:02:36,592 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.38:43911
2025-03-17 00:02:36,592 - distributed.worker - INFO -           Worker name:          SLURMCluster-19-0
2025-03-17 00:02:36,592 - distributed.worker - INFO -          dashboard at:          10.224.1.38:34155
2025-03-17 00:02:36,592 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:33455
2025-03-17 00:02:36,592 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:02:36,592 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:02:36,593 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:02:36,593 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-kxepwrm5
2025-03-17 00:02:36,593 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:02:36,598 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.38:40949
2025-03-17 00:02:36,598 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.38:40949
2025-03-17 00:02:36,598 - distributed.worker - INFO -           Worker name:          SLURMCluster-19-3
2025-03-17 00:02:36,598 - distributed.worker - INFO -          dashboard at:          10.224.1.38:36605
2025-03-17 00:02:36,598 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:33455
2025-03-17 00:02:36,599 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:02:36,599 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:02:36,599 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:02:36,599 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-bfd7thkf
2025-03-17 00:02:36,599 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:02:36,792 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.38:35031
2025-03-17 00:02:36,792 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.38:35031
2025-03-17 00:02:36,792 - distributed.worker - INFO -           Worker name:          SLURMCluster-19-2
2025-03-17 00:02:36,793 - distributed.worker - INFO -          dashboard at:          10.224.1.38:33001
2025-03-17 00:02:36,793 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:33455
2025-03-17 00:02:36,793 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:02:36,792 - distributed.worker - INFO -       Start worker at:    tcp://10.224.1.38:35097
2025-03-17 00:02:36,793 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:02:36,793 - distributed.worker - INFO -          Listening to:    tcp://10.224.1.38:35097
2025-03-17 00:02:36,793 - distributed.worker - INFO -           Worker name:          SLURMCluster-19-1
2025-03-17 00:02:36,793 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:02:36,793 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-1wecw735
2025-03-17 00:02:36,793 - distributed.worker - INFO -          dashboard at:          10.224.1.38:39297
2025-03-17 00:02:36,793 - distributed.worker - INFO - Waiting to connect to:   tcp://128.93.170.2:33455
2025-03-17 00:02:36,793 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:02:36,793 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:02:36,793 - distributed.worker - INFO -               Threads:                          1
2025-03-17 00:02:36,793 - distributed.worker - INFO -                Memory:                   0.93 GiB
2025-03-17 00:02:36,793 - distributed.worker - INFO -       Local Directory: /tmp/dask-scratch-space-672848/worker-k8_ap1vo
2025-03-17 00:02:36,794 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:02:37,143 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:02:37,144 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:33455
2025-03-17 00:02:37,144 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:02:37,144 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:02:37,144 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:33455
2025-03-17 00:02:37,145 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:33455
2025-03-17 00:02:37,145 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:02:37,146 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:33455
2025-03-17 00:02:37,205 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:02:37,206 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:33455
2025-03-17 00:02:37,206 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:02:37,206 - distributed.worker - INFO - Starting Worker plugin shuffle
2025-03-17 00:02:37,206 - distributed.worker - INFO -         Registered to:   tcp://128.93.170.2:33455
2025-03-17 00:02:37,206 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:33455
2025-03-17 00:02:37,207 - distributed.worker - INFO - -------------------------------------------------
2025-03-17 00:02:37,207 - distributed.core - INFO - Starting established connection to tcp://128.93.170.2:33455
slurmstepd-node038: error: *** JOB 3936128 ON node038 CANCELLED AT 2025-03-17T00:21:42 ***
